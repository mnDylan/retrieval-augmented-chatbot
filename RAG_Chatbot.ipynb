{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RCTaU_YXf0oP",
        "poCgl9xJL6jH"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPBtQrSrLtM4ErUYOxszsOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnDylan/retrieval-augmented-chatbot/blob/main/RAG_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAG CHATBOT (VICUNA)**"
      ],
      "metadata": {
        "id": "quBlSkW0gxy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 // Download Data**"
      ],
      "metadata": {
        "id": "RVxKYnoMeshz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1lWuq0COKnU9mCfMvTEq54DBLgAh3yYDx/view?usp=drive_link\n",
        "!gdown 1lWuq0COKnU9mCfMvTEq54DBLgAh3yYDx"
      ],
      "metadata": {
        "id": "PJBXyJwXerLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2 // Install and Import Libraries**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a9JWL-09c8Dd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58r4IL4yckHj"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.52.4\n",
        "!pip install -q bitsandbytes==0.46.0\n",
        "!pip install -q accelerate==1.7.0\n",
        "!pip install -q langchain==0.3.25\n",
        "!pip install -q langchainhub==0.1.21\n",
        "!pip install -q langchain-chroma==0.2.4\n",
        "!pip install -q langchain_experimental==0.3.4\n",
        "!pip install -q langchain-community==0.3.24\n",
        "!pip install -q langchain_huggingface==0.2.0\n",
        "!pip install -q python-dotenv==1.1.0\n",
        "!pip install -q pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub"
      ],
      "metadata": {
        "id": "YDcBmghxfosD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 // Read PDF File**\n"
      ],
      "metadata": {
        "id": "968LU1d9fjTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Loader = PyPDFLoader\n",
        "FILE_PATH = \"./YOLOv10_Tutorials.pdf\"\n",
        "loader = Loader(FILE_PATH)\n",
        "documents = loader.load()\n",
        "\n",
        "print(\"Number of documents: \", len(documents))\n",
        "type(documents)"
      ],
      "metadata": {
        "id": "G4FQJlP1fzUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Initialize text vectorization (embedding)**"
      ],
      "metadata": {
        "id": "Si4pIPe-f0_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"bkai-foundation-models/vietnamese-bi-encoder\"\n",
        ")"
      ],
      "metadata": {
        "id": "XWWCbyu1gVdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Initialize text splitter and split document**"
      ],
      "metadata": {
        "id": "zpIKbuHXf09C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "semantic_splitter = SemanticChunker(\n",
        "    embeddings=embeddings,\n",
        "    buffer_size=1,                          # NhÃ³m 3 cÃ¢u\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    breakpoint_threshold_amount=95,\n",
        "    # number_of_chunks= 30,                   # Sá»‘ chunks mong muá»‘n. TÃ¬m number_of_chunks - 1 Ä‘iá»ƒm cÃ³ similarity tháº¥p nháº¥t Ä‘á»ƒ cáº¯t. Náº¿u khÃ´ng Ä‘á»§ Ä‘iá»ƒm cáº¯t phÃ¹ há»£p â†’ sá»‘ chunks thá»±c táº¿ â‰  sá»‘ chunks mong muá»‘n\n",
        "    min_chunk_size=500,                     # Tá»‘i thiá»ƒu 1000 kÃ½ tá»±\n",
        "    # sentence_split_regex=r'(?<=[.?!â€¦])\\s+', # Bao gá»“m dáº¥u â€¦ cho tiáº¿ng Viá»‡t\n",
        "    add_start_index=True\n",
        ")\n",
        "\n",
        "churk = semantic_splitter.split_documents(documents)\n",
        "print(\"Number of semantic chunks: \", len(churk))"
      ],
      "metadata": {
        "id": "BEO7D__XgWYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(churk[0])\n",
        "print()\n",
        "print(churk[1])\n",
        "print()\n",
        "print(churk[2])"
      ],
      "metadata": {
        "id": "ITwlrur4gZJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Create vector database and retriever**"
      ],
      "metadata": {
        "id": "hnruwykMf05b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = Chroma.from_documents(documents=churk,\n",
        "                                  embedding=embeddings)\n",
        "\n",
        "retriever = vector_db.as_retriever()\n",
        "\n",
        "QUERY = \"YOLOv10 dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬\"\n",
        "result = retriever.invoke(QUERY)\n",
        "\n",
        "print(\"Number of relevant documents: \", len(result))\n"
      ],
      "metadata": {
        "id": "Kn8HPITKggT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Load LLMs (Vicuna)**"
      ],
      "metadata": {
        "id": "RCTaU_YXf0oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "nf4_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=nf4_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=model_pipeline,\n",
        ")"
      ],
      "metadata": {
        "id": "CWmi1WpCgkHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Prompting with RAG**"
      ],
      "metadata": {
        "id": "Dk5-8TCTgNSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "USER_QUESTION = \"YOLOv10 lÃ  gÃ¬?\"\n",
        "output = rag_chain.invoke(USER_QUESTION)\n",
        "output"
      ],
      "metadata": {
        "id": "3fJQrL2ggnOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STREAMLIT UI**"
      ],
      "metadata": {
        "id": "OkaEG6fgs-_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Install Library % SETUP Tunnel and RUN FILE*"
      ],
      "metadata": {
        "id": "poCgl9xJL6jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit==1.46.0"
      ],
      "metadata": {
        "id": "Ec13ybxygohE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ],
      "metadata": {
        "id": "xkZhnADQMPbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "R1VUNDjBMQFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *STREAMLIT APP.PY*"
      ],
      "metadata": {
        "id": "wqs5gdUUMffs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import os\n",
        "import torch\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from transformers import BitsAndBytesConfig\n",
        "import time\n",
        "\n",
        "# Session state initialization\n",
        "if 'rag_chain' not in st.session_state:\n",
        "    st.session_state.rag_chain = None\n",
        "if 'models_loaded' not in st.session_state:\n",
        "    st.session_state.models_loaded = False\n",
        "if 'embeddings' not in st.session_state:\n",
        "    st.session_state.embeddings = None\n",
        "if 'llm' not in st.session_state:\n",
        "    st.session_state.llm = None\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "if 'pdf_processed' not in st.session_state:\n",
        "    st.session_state.pdf_processed = False\n",
        "if 'pdf_name' not in st.session_state:\n",
        "    st.session_state.pdf_name = \"\"\n",
        "\n",
        "\n",
        "# Functions\n",
        "@st.cache_resource\n",
        "def load_embeddings():\n",
        "    return HuggingFaceEmbeddings(model_name=\"bkai-foundation-models/vietnamese-bi-encoder\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "\n",
        "    MODEL_NAME = \"lmsys/vicuna-7b-v1.5\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,  # Hoáº·c load_in_8bit=True\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\"  # nf4 lÃ  lá»±a chá»n tá»‘t cho mÃ´ hÃ¬nh lá»›n\n",
        "    )\n",
        "\n",
        "    # Load model vá»›i quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    model_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    return HuggingFacePipeline(pipeline=model_pipeline)\n",
        "\n",
        "def process_pdf(uploaded_file):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        tmp_file_path = tmp_file.name\n",
        "\n",
        "    loader = PyPDFLoader(tmp_file_path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    semantic_splitter = SemanticChunker(\n",
        "        embeddings=st.session_state.embeddings,\n",
        "        buffer_size=1,\n",
        "        breakpoint_threshold_type=\"percentile\",\n",
        "        breakpoint_threshold_amount=95,\n",
        "        min_chunk_size=500,\n",
        "        add_start_index=True\n",
        "    )\n",
        "\n",
        "    docs = semantic_splitter.split_documents(documents)\n",
        "    vector_db = Chroma.from_documents(documents=docs, embedding=st.session_state.embeddings)\n",
        "    retriever = vector_db.as_retriever()\n",
        "\n",
        "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "    def format_docs(docs):\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    rag_chain = (\n",
        "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | st.session_state.llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    os.unlink(tmp_file_path)\n",
        "    return rag_chain, len(docs)\n",
        "\n",
        "def add_message(role, content):\n",
        "    \"\"\"ThÃªm tin nháº¯n vÃ o lá»‹ch sá»­ chat\"\"\"\n",
        "    st.session_state.chat_history.append({\n",
        "        \"role\": role,\n",
        "        \"content\": content,\n",
        "        \"timestamp\": time.time()\n",
        "    })\n",
        "\n",
        "def clear_chat():\n",
        "    \"\"\"XÃ³a lá»‹ch sá»­ chat\"\"\"\n",
        "    st.session_state.chat_history = []\n",
        "\n",
        "def display_chat():\n",
        "    \"\"\"Hiá»ƒn thá»‹ lá»‹ch sá»­ chat\"\"\"\n",
        "    if st.session_state.chat_history:\n",
        "        for message in st.session_state.chat_history:\n",
        "            if message[\"role\"] == \"user\":\n",
        "                with st.chat_message(\"user\"):\n",
        "                    st.write(message[\"content\"])\n",
        "            else:\n",
        "                with st.chat_message(\"assistant\"):\n",
        "                    st.write(message[\"content\"])\n",
        "    else:\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.write(\"Xin chÃ o! TÃ´i lÃ  AI assistant. HÃ£y upload file PDF vÃ  báº¯t Ä‘áº§u Ä‘áº·t cÃ¢u há»i vá» ná»™i dung tÃ i liá»‡u nhÃ©! ðŸ˜Š\")\n",
        "\n",
        "# USER INTERFACE\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"PDF RAG Chatbot\",\n",
        "        layout=\"wide\",\n",
        "        initial_sidebar_state=\"expanded\"\n",
        "    )\n",
        "    st.title(\"PDF RAG Assistant\")\n",
        "    st.logo(\"./logo.png\", size=\"large\")\n",
        "\n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.title(\"âš™ï¸ CÃ i Ä‘áº·t\")\n",
        "\n",
        "        # Load models\n",
        "        if not st.session_state.models_loaded:\n",
        "            st.warning(\"â³ Äang táº£i models...\")\n",
        "            with st.spinner(\"Äang táº£i AI models...\"):\n",
        "                st.session_state.embeddings = load_embeddings()\n",
        "                st.session_state.llm = load_llm()\n",
        "                st.session_state.models_loaded = True\n",
        "            st.success(\"âœ… Models Ä‘Ã£ sáºµn sÃ ng!\")\n",
        "            st.rerun()\n",
        "        else:\n",
        "            st.success(\"âœ… Models Ä‘Ã£ sáºµn sÃ ng!\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "\n",
        "        # Upload PDF\n",
        "        st.subheader(\"ðŸ“„ Upload tÃ i liá»‡u\")\n",
        "        uploaded_file = st.file_uploader(\"Chá»n file PDF\", type=\"pdf\")\n",
        "\n",
        "        if uploaded_file:\n",
        "            if st.button(\"ðŸ”„ Xá»­ lÃ½ PDF\", use_container_width=True):\n",
        "                with st.spinner(\"Äang xá»­ lÃ½ PDF...\"):\n",
        "                    st.session_state.rag_chain, num_chunks = process_pdf(uploaded_file)\n",
        "                    st.session_state.pdf_processed = True\n",
        "                    st.session_state.pdf_name = uploaded_file.name\n",
        "                    # Reset chat history khi upload PDF má»›i\n",
        "                    clear_chat()\n",
        "                    add_message(\"assistant\", f\"âœ… ÄÃ£ xá»­ lÃ½ thÃ nh cÃ´ng file **{uploaded_file.name}**!\\n\\nðŸ“Š TÃ i liá»‡u Ä‘Æ°á»£c chia thÃ nh {num_chunks} pháº§n. Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u Ä‘áº·t cÃ¢u há»i vá» ná»™i dung tÃ i liá»‡u.\")\n",
        "                st.rerun()\n",
        "\n",
        "        # PDF status\n",
        "        if st.session_state.pdf_processed:\n",
        "            st.success(f\"ðŸ“„ ÄÃ£ táº£i: {st.session_state.pdf_name}\")\n",
        "        else:\n",
        "            st.info(\"ðŸ“„ ChÆ°a cÃ³ tÃ i liá»‡u\")\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "\n",
        "        # Chat controls\n",
        "        st.subheader(\"ðŸ’¬ Äiá»u khiá»ƒn Chat\")\n",
        "        if st.button(\"ðŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat\", use_container_width=True):\n",
        "            clear_chat()\n",
        "            st.rerun()\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "\n",
        "        # Instructions\n",
        "        st.subheader(\"ðŸ“‹ HÆ°á»›ng dáº«n\")\n",
        "        st.markdown(\"\"\"\n",
        "        **CÃ¡ch sá»­ dá»¥ng:**\n",
        "        1. **Upload PDF** - Chá»n file vÃ  nháº¥n \"Xá»­ lÃ½ PDF\"\n",
        "        2. **Äáº·t cÃ¢u há»i** - Nháº­p cÃ¢u há»i trong Ã´ chat\n",
        "        3. **Nháº­n tráº£ lá»i** - AI sáº½ tráº£ lá»i dá»±a trÃªn ná»™i dung PDF\n",
        "        \"\"\")\n",
        "\n",
        "    # Main content\n",
        "    st.markdown(\"*TrÃ² chuyá»‡n vá»›i Chatbot Ä‘á»ƒ trao Ä‘á»•i vá» ná»™i dung tÃ i liá»‡u PDF cá»§a báº¡n*\")\n",
        "\n",
        "    # Chat container\n",
        "    chat_container = st.container()\n",
        "\n",
        "    with chat_container:\n",
        "        # Display chat history\n",
        "        display_chat()\n",
        "\n",
        "    # Chat input\n",
        "    if st.session_state.models_loaded:\n",
        "        if st.session_state.pdf_processed:\n",
        "            # User input\n",
        "            user_input = st.chat_input(\"Nháº­p cÃ¢u há»i cá»§a báº¡n...\")\n",
        "\n",
        "            if user_input:\n",
        "                # Add user message\n",
        "                add_message(\"user\", user_input)\n",
        "\n",
        "                # Display user message immediately\n",
        "                with st.chat_message(\"user\"):\n",
        "                    st.write(user_input)\n",
        "\n",
        "                # Generate response\n",
        "                with st.chat_message(\"assistant\"):\n",
        "                    with st.spinner(\"Äang suy nghÄ©...\"):\n",
        "                        try:\n",
        "                            output = st.session_state.rag_chain.invoke(user_input)\n",
        "                            # Clean up the response\n",
        "                            if 'Answer:' in output:\n",
        "                                answer = output.split('Answer:')[1].strip()\n",
        "                            else:\n",
        "                                answer = output.strip()\n",
        "\n",
        "                            # Display response\n",
        "                            st.write(answer)\n",
        "\n",
        "                            # Add assistant message to history\n",
        "                            add_message(\"assistant\", answer)\n",
        "\n",
        "                        except Exception as e:\n",
        "                            error_msg = f\"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}\"\n",
        "                            st.error(error_msg)\n",
        "                            add_message(\"assistant\", error_msg)\n",
        "        else:\n",
        "            st.info(\"ðŸ”„ Vui lÃ²ng upload vÃ  xá»­ lÃ½ file PDF trÆ°á»›c khi báº¯t Ä‘áº§u chat!\")\n",
        "            st.chat_input(\"Nháº­p cÃ¢u há»i cá»§a báº¡n...\", disabled=True)\n",
        "    else:\n",
        "        st.info(\"â³ Äang táº£i AI models, vui lÃ²ng Ä‘á»£i...\")\n",
        "        st.chat_input(\"Nháº­p cÃ¢u há»i cá»§a báº¡n...\", disabled=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "BaG7y7qYMdZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}